# kaggle-初次我用原始数据三个模型测试下来，决策树交叉验证最高得0.771740014753702，测试得0.800298899506362，随机森林交叉验证最高可得0.8454419961512567，测试可得0.862786268665411，xgboost交叉验证最高可得0.8481398522853851，测试可得0.76优化后xgboost可得0.87795531153678与0.8558，第二次我将房价对数化，使房价分布更加服从正太话，让机器学习不受极大房价影响，而后决策树交叉验证最高得0.7663779128686193,测试0.7607229009814064，随机森林0.8612665961844307和0.8793912487550684，xgboost0.8945463311226318，0.8949¶
